I did this for apple back in January


import pandas as pd
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

data = pd.read_csv('HistoricalData_Apple1.csv')
data['Close/Last'] = pd.to_numeric(data['Close/Last'].str.replace('$', ''), errors='coerce')
data['Open'] = pd.to_numeric(data['Open'].str.replace('$', ''), errors='coerce')
data['High'] = pd.to_numeric(data['High'].str.replace('$', ''), errors='coerce')
data['Low'] = pd.to_numeric(data['Low'].str.replace('$', ''), errors='coerce')

# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Time Series Cross-Validation
tscv = TimeSeriesSplit(n_splits=5)

# Hyperparameters to tune
param_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1],
    'max_depth': [3, 5]
}

# Model with GridSearchCV for hyperparameter tuning
model = GridSearchCV(GradientBoostingRegressor(random_state=42), param_grid, cv=tscv, scoring='neg_mean_squared_error')
model.fit(X_train, y_train)


best_model = model.best_estimator_

predicted_returns = best_model.predict(X_test)

# Calculate VaR on predicted returns
confidence_level = 0.95
predicted_VaR = np.percentile(predicted_returns, (1 - confidence_level) * 100)

# Evaluate model
mse = mean_squared_error(y_test, predicted_returns)
r2 = r2_score(y_test, predicted_returns)

print("Mean Squared Error:", mse)
print("R-squared:", r2)
print(f"Predicted VaR at {confidence_level * 100}%: {-predicted_VaR*100}%")
print("Best Model Parameters:", model.best_params_)
